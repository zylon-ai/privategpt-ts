/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as core from '../../../../core';
import * as PrivategptApi from '../../..';
import * as serializers from '../../../../serialization';
import urlJoin from 'url-join';
import * as errors from '../../../../errors';

export declare namespace ContextChunks {
  interface Options {
    environment: core.Supplier<string>;
  }

  interface RequestOptions {
    timeoutInSeconds?: number;
    maxRetries?: number;
  }
}

export class ContextChunks {
  constructor(protected readonly _options: ContextChunks.Options) {}

  /**
   * Given a `text`, returns the most relevant chunks from the ingested documents.
   *
   * The returned information can be used to generate prompts that can be
   * passed to `/completions` or `/chat/completions` APIs. Note: it is usually a very
   * fast API, because only the Embeddings model is involved, not the LLM. The
   * returned information contains the relevant chunk `text` together with the source
   * `document` it is coming from. It also contains a score that can be used to
   * compare different results.
   *
   * The max number of chunks to be returned is set using the `limit` param.
   *
   * Previous and next chunks (pieces of text that appear right before or after in the
   * document) can be fetched by using the `prev_next_chunks` field.
   *
   * The documents being used can be filtered using the `context_filter` and passing
   * the document IDs to be used. Ingested documents IDs can be found using
   * `/ingest/list` endpoint. If you want all ingested documents to be used,
   * remove `context_filter` altogether.
   * @throws {@link PrivategptApi.UnprocessableEntityError}
   *
   * @example
   *     await privategptApi.contextChunks.chunksRetrieval({
   *         text: "string",
   *         contextFilter: {}
   *     })
   */
  public async chunksRetrieval(
    request: PrivategptApi.ChunksBody,
    requestOptions?: ContextChunks.RequestOptions,
  ): Promise<PrivategptApi.ChunksResponse> {
    const _response = await core.fetcher({
      url: urlJoin(
        await core.Supplier.get(this._options.environment),
        'v1/chunks',
      ),
      method: 'POST',
      headers: {
        'X-Fern-Language': 'JavaScript',
      },
      contentType: 'application/json',
      body: await serializers.ChunksBody.jsonOrThrow(request, {
        unrecognizedObjectKeys: 'strip',
      }),
      timeoutMs:
        requestOptions?.timeoutInSeconds != null
          ? requestOptions.timeoutInSeconds * 1000
          : 60000,
      maxRetries: requestOptions?.maxRetries,
    });
    if (_response.ok) {
      return await serializers.ChunksResponse.parseOrThrow(_response.body, {
        unrecognizedObjectKeys: 'passthrough',
        allowUnrecognizedUnionMembers: true,
        allowUnrecognizedEnumValues: true,
        breadcrumbsPrefix: ['response'],
      });
    }

    if (_response.error.reason === 'status-code') {
      switch (_response.error.statusCode) {
        case 422:
          throw new PrivategptApi.UnprocessableEntityError(
            await serializers.HttpValidationError.parseOrThrow(
              _response.error.body,
              {
                unrecognizedObjectKeys: 'passthrough',
                allowUnrecognizedUnionMembers: true,
                allowUnrecognizedEnumValues: true,
                breadcrumbsPrefix: ['response'],
              },
            ),
          );
        default:
          throw new errors.PrivategptApiError({
            statusCode: _response.error.statusCode,
            body: _response.error.body,
          });
      }
    }

    switch (_response.error.reason) {
      case 'non-json':
        throw new errors.PrivategptApiError({
          statusCode: _response.error.statusCode,
          body: _response.error.rawBody,
        });
      case 'timeout':
        throw new errors.PrivategptApiTimeoutError();
      case 'unknown':
        throw new errors.PrivategptApiError({
          message: _response.error.errorMessage,
        });
    }
  }
}
