/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as PrivategptApi from '../../..';
import * as core from '../../../../core';
import * as errors from '../../../../errors';
import * as serializers from '../../../../serialization';

import urlJoin from 'url-join';

export declare namespace ContextualCompletions {
  interface Options {
    environment: core.Supplier<string>;
  }

  interface RequestOptions {
    timeoutInSeconds?: number;
    maxRetries?: number;
  }
}

type ChatBody = Omit<PrivategptApi.ChatBody, 'stream'>;
type PromptBody = Omit<PrivategptApi.CompletionsBody, 'stream'>;

type RequestOptionsStreaming = Omit<
  ContextualCompletions.RequestOptions,
  'stream'
>;
type RequestOptionsNotStreaming = Omit<
  ContextualCompletions.RequestOptions,
  'stream'
> & { stream: false };

export class ContextualCompletions {
  constructor(protected readonly _options: ContextualCompletions.Options) {}

  /**
   * We recommend most users use our Chat completions API.
   *
   * Given a prompt, the model will return one predicted completion.
   *
   * Optionally include a `system_prompt` to influence the way the LLM answers.
   *
   * If `use_context`
   * is set to `true`, the model will use context coming from the ingested documents
   * to create the response. The documents being used can be filtered using the
   * `context_filter` and passing the document IDs to be used. Ingested documents IDs
   * can be found using `/ingest/list` endpoint. If you want all ingested documents to
   * be used, remove `context_filter` altogether.
   *
   * When using `'include_sources': true`, the API will return the source Chunks used
   * to create the response, which come from the context provided.
   *
   * ```
   * {"id":"12345","object":"completion.chunk","created":1694268190,
   * "model":"private-gpt","choices":[{"index":0,"delta":{"content":"Hello"},
   * "finish_reason":null}]}
   * ```
   * @throws {@link PrivategptApi.UnprocessableEntityError}
   *
   * @example
   *     await privategptApi.contextualCompletions.promptCompletionStream({
   *         prompt: "string",
   *         contextFilter: {}
   *     })
   */
  public async promptCompletionStream(
    request: PromptBody,
    requestOptions?: ContextualCompletions.RequestOptions,
    abortSignal?: AbortSignal,
  ): Promise<core.Stream<PrivategptApi.OpenAiCompletion>> {
    const _response = await core.fetcher({
      abortSignal,
      url: urlJoin(
        await core.Supplier.get(this._options.environment),
        'v1/completions',
      ),
      method: 'POST',
      headers: {
        'X-Fern-Language': 'JavaScript',
      },
      contentType: 'application/json',
      body: await serializers.CompletionsBody.jsonOrThrow(
        { ...request, stream: true },
        {
          unrecognizedObjectKeys: 'strip',
        },
      ),
      timeoutMs:
        requestOptions?.timeoutInSeconds != null
          ? requestOptions.timeoutInSeconds * 1000
          : 60000,
      maxRetries: requestOptions?.maxRetries,
      responseType: 'streaming',
    });
    if (_response.ok) {
      const stream = new core.Stream<PrivategptApi.OpenAiCompletion>({
        // @ts-ignore
        stream: _response.body,
        terminator: '\n',
        parse: async (data) => {
          return await serializers.OpenAiCompletion.parseOrThrow(data, {
            unrecognizedObjectKeys: 'passthrough',
            allowUnrecognizedUnionMembers: true,
            allowUnrecognizedEnumValues: true,
            skipValidation: true,
            breadcrumbsPrefix: ['response'],
          });
        },
      });
      return stream;
    }

    if (_response.error.reason === 'status-code') {
      switch (_response.error.statusCode) {
        case 422:
          throw new PrivategptApi.UnprocessableEntityError(
            await serializers.HttpValidationError.parseOrThrow(
              _response.error.body,
              {
                unrecognizedObjectKeys: 'passthrough',
                allowUnrecognizedUnionMembers: true,
                allowUnrecognizedEnumValues: true,
                breadcrumbsPrefix: ['response'],
              },
            ),
          );
        default:
          throw new errors.PrivategptApiError({
            statusCode: _response.error.statusCode,
            body: _response.error.body,
          });
      }
    }

    switch (_response.error.reason) {
      case 'non-json':
        throw new errors.PrivategptApiError({
          statusCode: _response.error.statusCode,
          body: _response.error.rawBody,
        });
      case 'timeout':
        throw new errors.PrivategptApiTimeoutError();
      case 'unknown':
        throw new errors.PrivategptApiError({
          message: _response.error.errorMessage,
        });
    }
  }

  /**
   * We recommend most users use our Chat completions API.
   *
   * Given a prompt, the model will return one predicted completion.
   *
   * Optionally include a `system_prompt` to influence the way the LLM answers.
   *
   * If `use_context`
   * is set to `true`, the model will use context coming from the ingested documents
   * to create the response. The documents being used can be filtered using the
   * `context_filter` and passing the document IDs to be used. Ingested documents IDs
   * can be found using `/ingest/list` endpoint. If you want all ingested documents to
   * be used, remove `context_filter` altogether.
   *
   * When using `'include_sources': true`, the API will return the source Chunks used
   * to create the response, which come from the context provided.
   *
   * ```
   * {"id":"12345","object":"completion.chunk","created":1694268190,
   * "model":"private-gpt","choices":[{"index":0,"delta":{"content":"Hello"},
   * "finish_reason":null}]}
   * ```
   * @throws {@link PrivategptApi.UnprocessableEntityError}
   *
   * @example
   *     await privategptApi.contextualCompletions.promptCompletion({
   *         prompt: "string",
   *         contextFilter: {}
   *     })
   */
  public async promptCompletion(
    request: PromptBody,
    requestOptions?: ContextualCompletions.RequestOptions,
  ): Promise<PrivategptApi.OpenAiCompletion> {
    const _response = await core.fetcher({
      url: urlJoin(
        await core.Supplier.get(this._options.environment),
        'v1/completions',
      ),
      method: 'POST',
      headers: {
        'X-Fern-Language': 'JavaScript',
      },
      contentType: 'application/json',
      body: await serializers.CompletionsBody.jsonOrThrow(request, {
        unrecognizedObjectKeys: 'strip',
      }),
      timeoutMs:
        requestOptions?.timeoutInSeconds != null
          ? requestOptions.timeoutInSeconds * 1000
          : 60000,
      maxRetries: requestOptions?.maxRetries,
      responseType: 'json',
    });
    if (_response.ok) {
      return await serializers.OpenAiCompletion.parseOrThrow(_response.body, {
        unrecognizedObjectKeys: 'passthrough',
        allowUnrecognizedUnionMembers: true,
        allowUnrecognizedEnumValues: true,
        breadcrumbsPrefix: ['response'],
      });
    }

    if (_response.error.reason === 'status-code') {
      switch (_response.error.statusCode) {
        case 422:
          throw new PrivategptApi.UnprocessableEntityError(
            await serializers.HttpValidationError.parseOrThrow(
              _response.error.body,
              {
                unrecognizedObjectKeys: 'passthrough',
                allowUnrecognizedUnionMembers: true,
                allowUnrecognizedEnumValues: true,
                breadcrumbsPrefix: ['response'],
              },
            ),
          );
        default:
          throw new errors.PrivategptApiError({
            statusCode: _response.error.statusCode,
            body: _response.error.body,
          });
      }
    }

    switch (_response.error.reason) {
      case 'non-json':
        throw new errors.PrivategptApiError({
          statusCode: _response.error.statusCode,
          body: _response.error.rawBody,
        });
      case 'timeout':
        throw new errors.PrivategptApiTimeoutError();
      case 'unknown':
        throw new errors.PrivategptApiError({
          message: _response.error.errorMessage,
        });
    }
  }

  /**
   * Given a list of messages comprising a conversation, return a response.
   *
   * Optionally include an initial `role: system` message to influence the way
   * the LLM answers.
   *
   * If `use_context` is set to `true`, the model will use context coming
   * from the ingested documents to create the response. The documents being used can
   * be filtered using the `context_filter` and passing the document IDs to be used.
   * Ingested documents IDs can be found using `/ingest/list` endpoint. If you want
   * all ingested documents to be used, remove `context_filter` altogether.
   *
   * When using `'include_sources': true`, the API will return the source Chunks used
   * to create the response, which come from the context provided.
   *
   * ```
   * {"id":"12345","object":"completion.chunk","created":1694268190,
   * "model":"private-gpt","choices":[{"index":0,"delta":{"content":"Hello"},
   * "finish_reason":null}]}
   * ```
   * @throws {@link PrivategptApi.UnprocessableEntityError}
   *
   * @example
   *     await privategptApi.contextualCompletions.chatCompletion({
   *         messages: [],
   *         contextFilter: {}
   *     })
   */
  public async chatCompletionStream(
    request: ChatBody,
    requestOptions?: ContextualCompletions.RequestOptions,
    abortSignal?: AbortSignal,
  ): Promise<core.Stream<PrivategptApi.OpenAiCompletion>> {
    const _response = await core.fetcher({
      abortSignal,
      url: urlJoin(
        await core.Supplier.get(this._options.environment),
        'v1/chat/completions',
      ),
      method: 'POST',
      headers: {
        'X-Fern-Language': 'JavaScript',
      },
      contentType: 'application/json',
      body: await serializers.ChatBody.jsonOrThrow(
        { ...request, stream: true },
        {
          unrecognizedObjectKeys: 'strip',
        },
      ),
      timeoutMs:
        requestOptions?.timeoutInSeconds != null
          ? requestOptions.timeoutInSeconds * 1000
          : 60000,
      maxRetries: requestOptions?.maxRetries,
      responseType: 'streaming',
    });
    if (_response.ok) {
      const stream = new core.Stream<PrivategptApi.OpenAiCompletion>({
        // @ts-ignore
        stream: _response.body,
        terminator: '\n',
        parse: async (data) => {
          return await serializers.OpenAiCompletion.parseOrThrow(data, {
            unrecognizedObjectKeys: 'passthrough',
            allowUnrecognizedUnionMembers: true,
            allowUnrecognizedEnumValues: true,
            skipValidation: true,
            breadcrumbsPrefix: ['response'],
          });
        },
      });
      return stream;
    }

    if (_response.error.reason === 'status-code') {
      switch (_response.error.statusCode) {
        case 422:
          throw new PrivategptApi.UnprocessableEntityError(
            await serializers.HttpValidationError.parseOrThrow(
              _response.error.body,
              {
                unrecognizedObjectKeys: 'passthrough',
                allowUnrecognizedUnionMembers: true,
                allowUnrecognizedEnumValues: true,
                breadcrumbsPrefix: ['response'],
              },
            ),
          );
        default:
          throw new errors.PrivategptApiError({
            statusCode: _response.error.statusCode,
            body: _response.error.body,
          });
      }
    }

    switch (_response.error.reason) {
      case 'non-json':
        throw new errors.PrivategptApiError({
          statusCode: _response.error.statusCode,
          body: _response.error.rawBody,
        });
      case 'timeout':
        throw new errors.PrivategptApiTimeoutError();
      case 'unknown':
        throw new errors.PrivategptApiError({
          message: _response.error.errorMessage,
        });
    }
  }

  public async chatCompletion(
    request: ChatBody,
    requestOptions?: RequestOptionsNotStreaming | RequestOptionsStreaming,
  ): Promise<PrivategptApi.OpenAiCompletion> {
    const _response = await core.fetcher({
      url: urlJoin(
        await core.Supplier.get(this._options.environment),
        'v1/chat/completions',
      ),
      method: 'POST',
      headers: {
        'X-Fern-Language': 'JavaScript',
      },
      contentType: 'application/json',
      body: await serializers.ChatBody.jsonOrThrow(request, {
        unrecognizedObjectKeys: 'strip',
      }),
      timeoutMs:
        requestOptions?.timeoutInSeconds != null
          ? requestOptions.timeoutInSeconds * 1000
          : 60000,
      maxRetries: requestOptions?.maxRetries,
      responseType: 'json',
    });
    if (_response.ok) {
      return await serializers.OpenAiCompletion.parseOrThrow(_response.body, {
        unrecognizedObjectKeys: 'passthrough',
        allowUnrecognizedUnionMembers: true,
        allowUnrecognizedEnumValues: true,
        breadcrumbsPrefix: ['response'],
      });
    }

    if (_response.error.reason === 'status-code') {
      switch (_response.error.statusCode) {
        case 422:
          throw new PrivategptApi.UnprocessableEntityError(
            await serializers.HttpValidationError.parseOrThrow(
              _response.error.body,
              {
                unrecognizedObjectKeys: 'passthrough',
                allowUnrecognizedUnionMembers: true,
                allowUnrecognizedEnumValues: true,
                breadcrumbsPrefix: ['response'],
              },
            ),
          );
        default:
          throw new errors.PrivategptApiError({
            statusCode: _response.error.statusCode,
            body: _response.error.body,
          });
      }
    }

    switch (_response.error.reason) {
      case 'non-json':
        throw new errors.PrivategptApiError({
          statusCode: _response.error.statusCode,
          body: _response.error.rawBody,
        });
      case 'timeout':
        throw new errors.PrivategptApiTimeoutError();
      case 'unknown':
        throw new errors.PrivategptApiError({
          message: _response.error.errorMessage,
        });
    }
  }
}
